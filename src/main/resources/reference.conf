direct {
  batch.duration.seconds = 180
  final.files.number = 10
  partition.by.dimentions {
    event = ["day", "hour"]
    offsets = "sum_offsets"
  }
  kafka {
    group.id.prefix = "jaquet"
    starting.offset = "latest"
    rebalance.max.retries=10
    retry.backoff.ms=200
    refresh.leader.backoff.ms=500
  }
  spark{
    spark.hadoop.fs.s3a.access.key="xxx"
    spark.hadoop.fs.s3a.secret.key="ppp"
    spark.hadoop.fs.s3a.region="myRegion"
    spark.master = "yarn"
    spark.streaming.kafka.maxRatePerPartition=450
    spark.streaming.kafka.consumer.poll.ms=30000
    spark.app.name = "Jaquet"
    spark.yarn.submit.waitAppCompletion=false,
    spark.yarn.maxAppAttempts=4,
    spark.yarn.am.attemptFailuresValidityInterval=1h,
    spark.yarn.max.executor.failures=56,
    spark.yarn.executor.failuresValidityInterval=1h
    spark.sql.parquet.mergeSchema=false
    spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
    spark.speculation=false
    spark.hadoop.fs.s3a.fast.upload=true
  }

}


kafka {
  serversList = [
    {
      host="localhost",
      port=9092
    }
  ]
}

